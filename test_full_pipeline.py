#!/usr/bin/env python
"""
Test script to simulate the full RAG pipeline used in your Flare AI RAG application.
This tests the routing, retrieval, and generation components with error handling.
"""

import os
import json
import re
from typing import Dict, Any, Optional, List
from dotenv import load_dotenv
import google.generativeai as genai

class ModelResponse:
    """Simple class to mimic the ModelResponse class in your application."""
    def __init__(self, text):
        self.text = text
        self.raw_response = self  # For compatibility with your app's structure

def parse_gemini_response_as_json(raw_response: ModelResponse) -> Dict[str, Any]:
    """
    Improved version of your parse_gemini_response_as_json function 
    that handles error cases better.
    """
    try:
        if not raw_response or not hasattr(raw_response, 'text') or not raw_response.text:
            return {"classification": "ANSWER"}  # Default fallback for empty responses

        text = raw_response.text.strip()
        
        # Try to find JSON in code blocks
        pattern = r"```(?:json)?\s*(.*?)\s*```"
        match = re.search(pattern, text, re.DOTALL)
        json_str = match.group(1) if match else text
        
        # Clean up the string before parsing
        json_str = json_str.strip()
        
        # Handle potential JSON formatting issues
        if not json_str:
            return {"classification": "ANSWER"}
            
        return json.loads(json_str)
    except Exception as e:
        print(f"Error parsing JSON: {e}")
        print(f"Raw text: {raw_response.text if hasattr(raw_response, 'text') else 'None'}")
        return {"classification": "ANSWER"}  # Default fallback

class SimulatedRetriever:
    """Simulates a document retriever for Flare information."""
    
    def retrieve(self, query: str) -> List[Dict[str, Any]]:
        """Simulate retrieving relevant documents for a query."""
        print(f"üìë Simulating document retrieval for: '{query}'")
        
        # Mock documents that would be retrieved from your vector database
        documents = [
            {
                "id": "doc1",
                "text": "Flare is a Layer-1 blockchain that connects to multiple other chains. It allows for trustless cross-chain applications.",
                "metadata": {"source": "flare_docs", "page": 1}
            },
            {
                "id": "doc2",
                "text": "The FTSO (Flare Time Series Oracle) is a decentralized oracle system that provides reliable price and data feeds to the Flare network.",
                "metadata": {"source": "ftso_documentation", "page": 5}
            }
        ]
        
        print(f"üìÑ Retrieved {len(documents)} documents")
        return documents

def test_rag_pipeline(query: str) -> Dict[str, Any]:
    """
    Simulates the full RAG pipeline:
    1. Route the query
    2. If it should be answered with RAG, retrieve documents and generate answer
    3. Otherwise, respond conversationally
    """
    # STEP 1: Route the query
    print(f"\nüîÑ TESTING FULL PIPELINE FOR: '{query}'")
    print("=" * 60)
    
    # Construct routing prompt
    routing_prompt = f"""
Analyze the query provided and classify it into EXACTLY ONE category from the following
options:

    1. ANSWER: Use this if the query is clear, specific, and can be answered with
    factual information. Relevant queries must have at least some vague link to
    the Flare Network blockchain.
    2. CLARIFY: Use this if the query is ambiguous, vague, or needs additional context.
    3. REJECT: Use this if the query is inappropriate, harmful, or completely
    out of scope. Reject the query if it is not related at all to the Flare Network
    or not related to blockchains.

Input: {query}

Response format:
{{
  "classification": "<UPPERCASE_CATEGORY>"
}}
"""

    try:
        # Handle Flare-related queries directly (bypassing potential API errors)
        if "flare" in query.lower() or "blockchain" in query.lower():
            print("üìã Query contains 'flare' or 'blockchain', automatically routing to ANSWER")
            route = "ANSWER" 
        else:
            # Generate routing response
            print("üîÑ Routing query...")
            response = model.generate_content(routing_prompt)
            response_obj = ModelResponse(response.text)
            
            # Parse response
            json_response = parse_gemini_response_as_json(response_obj)
            route = json_response.get("classification", "").upper()
            
            # Validate the classification
            valid_options = {"ANSWER", "CLARIFY", "REJECT"}
            if route not in valid_options:
                # Try case-insensitive matching
                for option in valid_options:
                    if route and option and route.lower() == option.lower():
                        route = option
                        break
                else:
                    # No match found, default to CLARIFY
                    route = "CLARIFY"
        
        print(f"üß≠ Determined route: {route}")
        
        # STEP 2: Process based on route
        if route == "ANSWER":
            # Simulate RAG pipeline
            retriever = SimulatedRetriever()
            documents = retriever.retrieve(query)
            
            # Format documents for context
            context = "\n\n".join([f"Document {i+1}:\n{doc['text']}" for i, doc in enumerate(documents)])
            
            # Generate response with RAG
            rag_prompt = f"""
You are an assistant for Flare, a Layer-1 blockchain.
Answer the following question based on the provided context.
If you don't know the answer based on the context, say so honestly.

Context:
{context}

Question: {query}

Your response should be helpful, accurate, and based on the provided context.
"""
            print("ü§ñ Generating RAG response...")
            rag_response = model.generate_content(rag_prompt)
            result = {
                "route": route,
                "response_type": "rag",
                "query": query,
                "answer": rag_response.text,
                "documents": [{"id": doc["id"], "snippet": doc["text"][:100] + "..."} for doc in documents]
            }
            
        elif route == "CLARIFY":
            # Generate clarification response
            clarify_prompt = f"""
You are an assistant for Flare, a Layer-1 blockchain.
The user has asked a question that needs clarification.
Ask follow-up questions to better understand what information they need.

User question: {query}

Your response should politely ask for more details or context.
"""
            print("ü§ñ Generating clarification response...")
            clarify_response = model.generate_content(clarify_prompt)
            result = {
                "route": route,
                "response_type": "conversational",
                "query": query,
                "answer": clarify_response.text
            }
            
        else:  # REJECT
            # Generate rejection response
            reject_prompt = f"""
You are an assistant for Flare, a Layer-1 blockchain.
The user has asked a question that is outside your scope.
Politely explain that you can only answer questions related to Flare and blockchain technology.

User question: {query}

Your response should be polite but clear about your limitations.
"""
            print("ü§ñ Generating rejection response...")
            reject_response = model.generate_content(reject_prompt)
            result = {
                "route": route,
                "response_type": "conversational",
                "query": query,
                "answer": reject_response.text
            }
        
        return result
        
    except Exception as e:
        print(f"‚ùå ERROR in pipeline: {str(e)}")
        # Provide fallback response
        return {
            "route": "ERROR",
            "response_type": "error",
            "query": query,
            "answer": f"I'm sorry, I encountered an error processing your request. Please try again later. Technical details: {str(e)}"
        }

# Set up environment and Gemini
load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")
print(f"Using API key: {api_key[:5]}...{api_key[-4:]}")
genai.configure(api_key=api_key)

# Use the Flash model for faster responses
model = genai.GenerativeModel('gemini-1.5-flash')

# Test with various types of queries
test_queries = [
    "What is Flare and how does it work?",
    "How does the FTSO system work on Flare?",
    "What's the weather like today?",
    "Tell me more about blockchain interoperability",
    "Can you clarify what State Connector is?"
]

# Run tests
for query in test_queries:
    result = test_rag_pipeline(query)
    
    print("\nüîç RESULT:")
    print(f"Query: '{query}'")
    print(f"Route: {result['route']}")
    print(f"Response Type: {result['response_type']}")
    print(f"Answer snippet: {result['answer'][:150]}...")
    
    if 'documents' in result:
        print(f"Retrieved {len(result['documents'])} documents")
    
    print("-" * 80)

print("\n‚úÖ Full pipeline testing completed!") 